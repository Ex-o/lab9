{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:\n",
    "``graph:``The input graph.\n",
    "\n",
    "``initialMsg:``Received by each node on the 1st iteration.\n",
    "\n",
    "``maxIterations:``Number of iterations before the program terminates.\n",
    "\n",
    "``activeDirection:``Setting whether sendMsg will be ran in the next iteration or not.\n",
    "\n",
    "``vprog:``The function that is used to transform the nodes after each iteration. It runs on the nodes that receive the msg.\n",
    "\n",
    "``sendMsg:``The function that is ran on the out edges of the nodes that got a msg in the current iteration.\n",
    "\n",
    "``receiveMsg:``Merges two incoming msgs into one.\n",
    "\n",
    "\n",
    "# Task 2:\n",
    "1. Building efficient routes (e.g, network routes consisting of millions of nodes with as little complexity as possible).\n",
    "2. Recommendation systems based on the insights extracted from customer data analysis, e.g, product recommendations in e-commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://khaled:4040\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1638363344855)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,1.7002451224528383)\n",
      "(3,1.1294346981766874)\n",
      "(4,0.9190514175420748)\n",
      "(1,0.4390416708169825)\n",
      "(2,0.8122270910114175)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "myVertices: org.apache.spark.rdd.RDD[(Long, String)] = ParallelCollectionRDD[0] at makeRDD at <console>:25\n",
       "myEdges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1] at makeRDD at <console>:33\n",
       "myGraph: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@48169246\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "val myVertices = sc.makeRDD(Array(\n",
    "    (1L, \"Ann\"),\n",
    "    (2L, \"Bill\"),\n",
    "    (3L, \"Charles\"),\n",
    "    (4L, \"Diane\"),\n",
    "    (5L, \"Went to gym this morning\")\n",
    "))\n",
    "\n",
    "val myEdges = sc.makeRDD(Array(\n",
    "    Edge(1L, 2L, \"is-friends-with\"),\n",
    "    Edge(2L, 3L, \"is-friends-with\"),\n",
    "    Edge(3L, 4L, \"is-friends-with\"),\n",
    "    Edge(4L, 5L, \"Likes-status\"),\n",
    "    Edge(3L, 5L, \"Wrote-status\")\n",
    "))\n",
    "\n",
    "val myGraph = Graph(myVertices, myEdges)\n",
    "myGraph.pageRank(0.001).vertices.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a new edge from 4 -> 1 & 1 -> 5 and remove 4-> & 3 -> 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1.393293778383316)\n",
      "(2,0.8946623413364332)\n",
      "(4,0.7549827071549726)\n",
      "(5,0.8946623413364332)\n",
      "(3,1.0623988317888453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "myEdges2: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[131] at makeRDD at <console>:28\n",
       "myGraph2: org.apache.spark.graphx.Graph[String,String] = org.apache.spark.graphx.impl.GraphImpl@3d2dc319\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myEdges2 = sc.makeRDD(Array(\n",
    "    Edge(1L, 2L, \"is-friends-with\"),\n",
    "    Edge(2L, 3L, \"is-friends-with\"),\n",
    "    Edge(3L, 4L, \"is-friends-with\"),\n",
    "    Edge(3L, 1L, \"is-friends-with\"),\n",
    "    Edge(4L, 1L, \"is-friends-with\"),\n",
    "    Edge(1L, 5L, \"Wrote-status\")\n",
    "))\n",
    "\n",
    "val myGraph2 = Graph(myVertices, myEdges2)\n",
    "myGraph2.pageRank(0.001).vertices.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (1) is the most popular node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
